import reimport nltk.tag.stanford as SPTfrom nltk.stem import WordNetLemmatizerfrom nltk import word_tokenizefrom nltk.corpus import stopwords as STOP_PATH_TO_MODEL = 'C:/stanford/models/english-bidirectional-distsim.tagger'_PATH_TO_JAR = 'C:/stanford/stanford-postagger.jar'tweet = "RT the @EndorsementsGOP: Jeff Dewit R : @WeNeedTrump: Women4Trump #Trump2016 I don't not or don't back vote him #NYC https://t.co/GCJZGWOBcN #Trump #PA #GOP #DE.."positive_tags = ['WeNeedTrump','WomenTrumpmania','Women4Trump','MakeAmericaGreatAgain','Trump2016','TrumpTrain','AlwaysTrump']negative_tags = ['NeverTrump','AntiTrump','DuckingDonald']ALPHANUM = re.compile(r'(\d+|\s+)')COMBINEDWORD = re.compile(r'[A-Z][^A-Z]*')replace_dict = {    "don't": "do not",    "won't": "will not",    "didn't": "did not",    "doesn't": "does not"}def replaceNT(text):    for word in replace_dict:        if word in text:  # Small first letter            text = text.replace(word, replace_dict[word])        #elif word[0].title() + word[1:] in text:  # Big first letter        #   text = text.replace(word[0].title() + word[1:],        #                      replace_dict[word][0].title() + replace_dict[word][1:])    return text#start replaceTwoOrMoreChardef replaceTwoOrMoreChar(tweet):    #look for 2 or more repetitions of character and replace with the character itself    pattern = re.compile(r"(.)\1{1,}", re.DOTALL)    return pattern.sub(r"\1\1", tweet)#enddef preprocess(tweet):    tweet = re.sub('(https?://[^\s]+)','||URL||',tweet)    #Replace #Word with Word    tweet = re.sub(r'#([^\s]+)', r'\1', tweet)    #Replace @Word with Word    tweet = re.sub(r'@([^\s]+)', r'\1', tweet)    #Remove words that start with numbers    tweet = re.sub(r'^[0-9]*','',tweet)    #Replace RT at start of tweet    tweet = re.sub(r'^RT','',tweet)    #Replace two or more sequence of characters to two character    tweet = replaceTwoOrMoreChar(tweet)    #Replace n't with not     tweet = replaceNT(tweet)    #Remove more than space    tweet = re.sub('[\s]+', ' ', tweet)     return re.split(ALPHANUM,tweet)	
def par_speech_tags(tweet_split):    #Using stanford POS Tagger to Tag words, better than NLTK default tagger    tagger = SPT.StanfordPOSTagger(_PATH_TO_MODEL,_PATH_TO_JAR)    tagged = tagger.tag(tweet_split) 	    return taggeddef filter_tweet(cleand_tweet_s):    cleand_tweet = [] # Building a list of clear words    #Handling Combined words    for each_word in cleand_tweet_s:        isMatch = COMBINEDWORD.search(each_word)        if isMatch:            for each_word_1 in re.findall(COMBINEDWORD,each_word):                cleand_tweet.append(each_word_1)        else:            cleand_tweet.append(each_word)    # remove stopwords    #stopwords = STOP.words('english')    #filtrd_tweet_1 = [each_word.lower() for each_word in cleand_tweet if each_word.lower() not in stopwords]          #Lemmatize	    WORDNETLEMMA = WordNetLemmatizer()    print(cleand_tweet)    filtrd_tweet = [WORDNETLEMMA.lemmatize(each_word) for each_word in cleand_tweet]        return filtrd_tweetdef extrctPhases(filtrd_tweet):    return phrasesif __name__ == '__main__':    print("Cleaning The tweet:")    cleand_tweet = preprocess(tweet)    print(cleand_tweet)    print("Filtering The tweet:")    filtrd_tweet = filter_tweet(cleand_tweet)    print(filtrd_tweet)    print("POS Tagging The tweet:")    tagged_tweet = par_speech_tags(filtrd_tweet)    #extrctPhases()	    print(tagged_tweet)